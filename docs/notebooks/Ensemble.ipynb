{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll show how to perform a very simple ensemble analysis to infer the statistical properties of the spots on a group of stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Disable annoying font warnings\n",
    "matplotlib.font_manager._log.setLevel(50)\n",
    "\n",
    "# Disable theano deprecation warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=matplotlib.MatplotlibDeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"theano\")\n",
    "\n",
    "# Style\n",
    "plt.style.use(\"default\")\n",
    "plt.rcParams[\"savefig.dpi\"] = 100\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 4)\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Liberation Sans\"]\n",
    "plt.rcParams[\"font.cursive\"] = [\"Liberation Sans\"]\n",
    "try:\n",
    "    plt.rcParams[\"mathtext.fallback\"] = \"cm\"\n",
    "except KeyError:\n",
    "    plt.rcParams[\"mathtext.fallback_to_cm\"] = True\n",
    "plt.rcParams[\"mathtext.fallback_to_cm\"] = True\n",
    "\n",
    "# Short arrays when printing\n",
    "np.set_printoptions(threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "del matplotlib\n",
    "del plt\n",
    "del warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starry_process import StarryProcess, calibrate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import pymc3 as pm\n",
    "import pymc3_ext as pmx\n",
    "from corner import corner\n",
    "import theano\n",
    "import theano.tensor as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from corner import corner as _corner\n",
    "\n",
    "\n",
    "def corner(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Override `corner.corner` by making some appearance tweaks.\n",
    "\n",
    "    \"\"\"\n",
    "    # Get the usual corner plot\n",
    "    figure = _corner(*args, **kwargs)\n",
    "\n",
    "    # Get the axes\n",
    "    ndim = int(np.sqrt(len(figure.axes)))\n",
    "    axes = np.array(figure.axes).reshape((ndim, ndim))\n",
    "\n",
    "    # Smaller tick labels\n",
    "    for ax in axes[1:, 0]:\n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(10)\n",
    "        formatter = matplotlib.ticker.ScalarFormatter(useOffset=False)\n",
    "        ax.yaxis.set_major_formatter(formatter)\n",
    "        ax.set_ylabel(ax.get_ylabel(), fontsize=kwargs.get(\"corner_label_size\", 16))\n",
    "    for ax in axes[-1, :]:\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(10)\n",
    "        formatter = matplotlib.ticker.ScalarFormatter(useOffset=False)\n",
    "        ax.xaxis.set_major_formatter(formatter)\n",
    "        ax.set_xlabel(ax.get_xlabel(), fontsize=kwargs.get(\"corner_label_size\", 16))\n",
    "\n",
    "    # Pad the axes to always include the truths\n",
    "    truths = kwargs.get(\"truths\", None)\n",
    "    if truths is not None:\n",
    "        for row in range(1, ndim):\n",
    "            for col in range(row):\n",
    "                lo, hi = np.array(axes[row, col].get_xlim())\n",
    "                if truths[col] < lo:\n",
    "                    lo = truths[col] - 0.1 * (hi - truths[col])\n",
    "                    axes[row, col].set_xlim(lo, hi)\n",
    "                    axes[col, col].set_xlim(lo, hi)\n",
    "                elif truths[col] > hi:\n",
    "                    hi = truths[col] - 0.1 * (hi - truths[col])\n",
    "                    axes[row, col].set_xlim(lo, hi)\n",
    "                    axes[col, col].set_xlim(lo, hi)\n",
    "\n",
    "                lo, hi = np.array(axes[row, col].get_ylim())\n",
    "                if truths[row] < lo:\n",
    "                    lo = truths[row] - 0.1 * (hi - truths[row])\n",
    "                    axes[row, col].set_ylim(lo, hi)\n",
    "                    axes[row, row].set_xlim(lo, hi)\n",
    "                elif truths[row] > hi:\n",
    "                    hi = truths[row] - 0.1 * (hi - truths[row])\n",
    "                    axes[row, col].set_ylim(lo, hi)\n",
    "                    axes[row, row].set_xlim(lo, hi)\n",
    "\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a synthetic ensemble of light curves of stars with \"similar\" spot properties. Let's define some true values for the spot properties of the ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truths = {\"r\": 15, \"mu\": 30, \"sigma\": 5, \"c\": 0.05, \"n\": 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from starry_process.defaults import defaults\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        \"\"\"\n",
    "| parameter | description | true value\n",
    "| - | :- | :-:\n",
    "| `r` | mean radius in degrees | `{r}`\n",
    "| `mu` | latitude distribution mode in degrees | `{mu}`\n",
    "| `sigma` | latitude distribution standard deviation in degrees | `{sigma}`\n",
    "| `c` | fractional spot contrast | `{c}`\n",
    "| `n` | number of spots | `{n}`\n",
    "\"\"\".format(\n",
    "            **truths\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate 500 light curves from stars at random inclinations with spots drawn from the distributions above.\n",
    "We'll do this by adding discrete circular spots to each star via the `starry_process.calibrate.generate`\n",
    "function.\n",
    "Note that in order to mimic real observations, we'll normalize each light curve to its mean value and subtract unity to get the \"relative\" flux.\n",
    "For simplicity, we'll give all of the light curves the same period and photometric uncertainty."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "\n",
    "    The ``starry_process.calibrate`` module is used internally to verify the calibration of the Gaussian process, so it's not very well documented. You can check out the list of valid keyword arguments to the ``generate`` function `here <https://github.com/rodluger/starry_process/blob/ba90c7e7ff9a89939ad35ad331404f050027805d/starry_process/calibrate/defaults.json>`_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = calibrate.generate(\n",
    "    generate=dict(\n",
    "        normalized=True,\n",
    "        nlc=500,\n",
    "        period=1.0,\n",
    "        ferr=1e-3,\n",
    "        nspots=dict(mu=truths[\"n\"]),\n",
    "        radius=dict(mu=truths[\"r\"]),\n",
    "        latitude=dict(mu=truths[\"mu\"], sigma=truths[\"sigma\"]),\n",
    "        contrast=dict(mu=truths[\"c\"]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `data` is a dictionary containing the light curves, the stellar maps (expressed as vectors of spherical harmonic coefficients `y`), plus some metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data[\"t\"]\n",
    "flux = data[\"flux\"]\n",
    "ferr = data[\"ferr\"]\n",
    "y = data[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some of the light curves, all on the same scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 5)\n",
    "for j, axis in enumerate(ax.flatten()):\n",
    "    axis.plot(t, flux[j] * 1000)\n",
    "    axis.set_ylim(-50, 50)\n",
    "    axis.set_xticks([0, 1, 2, 3, 4])\n",
    "    if j != 10:\n",
    "        axis.set_xticklabels([])\n",
    "        axis.set_yticklabels([])\n",
    "    else:\n",
    "        axis.set_xlabel(\"rotations\")\n",
    "        axis.set_ylabel(\"flux [ppt]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll assume we observe only these 500 light curves. We do not know the inclinations of any of the stars or anything about their spot properties: only that all the stars have statistically similar spot distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a simple probabilistic model using `pymc3` and solve for the five quantities above: the spot radius, the mode and standard deviation of the spot latitude, the spot contrast, and the number of spots. We'll place uniform priors on everything except for the latitude mode `mu`, on which we'll place an isotropic prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "\n",
    "    # For use later\n",
    "    varnames = [\"r\", \"mu\", \"sigma\", \"c\", \"n\"]\n",
    "\n",
    "    # Spot latitude params. Isotropic prior on the mode\n",
    "    # and uniform prior on the standard deviation\n",
    "    u = pm.Uniform(\"u\", 0.0, 1.0)\n",
    "    mu = 90 - tt.arccos(u) * 180 / np.pi\n",
    "    pm.Deterministic(\"mu\", mu)\n",
    "    sigma = pm.Uniform(\"sigma\", 1.0, 20.0)\n",
    "\n",
    "    # Spot radius (uniform prior)\n",
    "    r = pm.Uniform(\"r\", 10.0, 30.0)\n",
    "\n",
    "    # Spot contrast & number of spots (uniform prior)\n",
    "    c = pm.Uniform(\"c\", 0.0, 1.0, testval=0.1)\n",
    "    n = pm.Uniform(\"n\", 1.0, 50.0, testval=5)\n",
    "\n",
    "    # Instantiate the GP\n",
    "    sp = StarryProcess(r=r, mu=mu, sigma=sigma, c=c, n=n)\n",
    "\n",
    "    # Compute the log likelihood\n",
    "    lnlike = sp.log_likelihood(t, flux, ferr ** 2, p=1.0)\n",
    "    pm.Potential(\"lnlike\", lnlike)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "\n",
    "   Note that we explicitly provide a small ``testval`` for both the spot contrast and the number of spots. Otherwise, the initial value assumed in the optimization for both those quantities is the midpoint of the bounds (`c = 0.5` and `n = 25.5`). That corresponds to a *lot* of *very dark* spots, which results in very high variance in the flux -- too high, in fact, for the normalized Gaussian process to model! We discusss this in more detail in the paper. To avoid initializing the sampler in a bad region of parameter space, we provide a starting point that is guaranteed to lead to a finite log likelihood value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could go on to do inference using `NUTS` or `ADVI` or any of the other samplers supported by `pymc3`. But that would take a few hours (at least). Since we have many light curves, we can actually get good point estimates by just optimizing the log probability function (which should take only a minute or two). We can then approximate the posterior using a Laplace approximation at the mode."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. warning::\n",
    "\n",
    "    When working with real data, you almost certainly want to use a proper sampling scheme! The estimate of the inverse of the Hessian, which we interpret as the posterior covariance, is often numerically unstable. The posterior could also be non-Gaussian or even multimodal. Our trick works here, but we only know that because we know what the true answer is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(varnames, model=None, nsamples=3000):\n",
    "    # Find the maximum a posteriori (MAP) point\n",
    "    soln, info = pmx.optimize(model=model, return_info=True)\n",
    "\n",
    "    # If it failed, abort\n",
    "    if info[\"nfev\"] == 1:\n",
    "        return None\n",
    "\n",
    "    # MAP solution and approximate covariance\n",
    "    map_mu = info[\"x\"]\n",
    "    map_cov = info[\"hess_inv\"]\n",
    "\n",
    "    # Draw samples from the Laplace approximation to the posterior\n",
    "    u = np.random.multivariate_normal(map_mu, map_cov, size=nsamples)\n",
    "\n",
    "    # We need to transform from the internal pymc3 parametrization\n",
    "    # (where all variables have infinite support)\n",
    "    # to our own parametrization (where the variables have\n",
    "    # priors with finite bounds). This is the easiest way\n",
    "    # of doing this!\n",
    "    samples = np.zeros_like(u)\n",
    "    wrapper = pmx.optim.ModelWrapper(model=model)\n",
    "    for k in tqdm(range(nsamples)):\n",
    "        point = pmx.optim.get_point(wrapper, u[k])\n",
    "        for j, name in enumerate(varnames):\n",
    "            samples[k, j] = point[name]\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = get_samples(varnames, model=model)\n",
    "corner(samples, labels=varnames, truths=[truths[name] for name in varnames]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! We correctly inferred *all* the hyperparameters of the GP! Note, importantly, that our constraint on the number of spots `n` is very poor. It is also very correlated with the spot contrast. Constraining the total number of spots, or the total spot coverage, from single-band photometric measurements is very difficult!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
