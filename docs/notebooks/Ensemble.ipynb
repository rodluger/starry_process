{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll show how to perform a very simple ensemble analysis to infer the statistical properties of the spots on a group of stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    get_ipython().run_line_magic(\"run\", \"notebook_config.py\")\n",
    "except:\n",
    "    import warnings\n",
    "\n",
    "    warnings.warn(\"Can't execute `notebook_config.py`.\")\n",
    "from IPython.display import display, Markdown\n",
    "from starry_process.defaults import defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will generate a synthetic ensemble of light curves of stars with \"similar\" spot properties. Let's define some true values for the spot properties of the ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truths = {\"r\": 15, \"mu\": 30, \"sigma\": 5, \"c\": 0.05, \"n\": 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        \"\"\"\n",
    "| parameter | description | true value\n",
    "| - | :- | :-:\n",
    "| `r` | mean radius in degrees | `{r}`\n",
    "| `mu` | latitude distribution mode in degrees | `{mu}`\n",
    "| `sigma` | latitude distribution standard deviation in degrees | `{sigma}`\n",
    "| `c` | fractional spot contrast | `{c}`\n",
    "| `n` | number of spots | `{n}`\n",
    "\"\"\".format(\n",
    "            **truths\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate 500 light curves from stars at random inclinations with spots drawn from the distributions above.\n",
    "We'll do this by adding discrete circular spots to each star via the `starry_process.calibrate.generate`\n",
    "function.\n",
    "Note that in order to mimic real observations, we'll normalize each light curve to its mean value and subtract unity to get the \"relative\" flux.\n",
    "For simplicity, we'll give all of the light curves the same period and photometric uncertainty."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "\n",
    "    The ``starry_process.calibrate`` module is used internally to verify the calibration of the Gaussian process, so it's not very well documented. You can check out the list of valid keyword arguments to the ``generate`` function `in this file <https://github.com/rodluger/starry_process/blob/ba90c7e7ff9a89939ad35ad331404f050027805d/starry_process/calibrate/defaults.json>`_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starry_process import calibrate\n",
    "\n",
    "data = calibrate.generate(\n",
    "    generate=dict(\n",
    "        normalized=True,\n",
    "        nlc=500,\n",
    "        period=1.0,\n",
    "        ferr=1e-3,\n",
    "        nspots=dict(mu=truths[\"n\"]),\n",
    "        radius=dict(mu=truths[\"r\"]),\n",
    "        latitude=dict(mu=truths[\"mu\"], sigma=truths[\"sigma\"]),\n",
    "        contrast=dict(mu=truths[\"c\"]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `data` is a dictionary containing the light curves, the stellar maps (expressed as vectors of spherical harmonic coefficients `y`), plus some metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data[\"t\"]\n",
    "flux = data[\"flux\"]\n",
    "ferr = data[\"ferr\"]\n",
    "y = data[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some of the light curves, all on the same scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(3, 5)\n",
    "for j, axis in enumerate(ax.flatten()):\n",
    "    axis.plot(t, flux[j] * 1000)\n",
    "    axis.set_ylim(-50, 50)\n",
    "    axis.set_xticks([0, 1, 2, 3, 4])\n",
    "    if j != 10:\n",
    "        axis.set_xticklabels([])\n",
    "        axis.set_yticklabels([])\n",
    "    else:\n",
    "        axis.set_xlabel(\"rotations\")\n",
    "        axis.set_ylabel(\"flux [ppt]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll assume we observe only these 500 light curves. We do not know the inclinations of any of the stars or anything about their spot properties: only that all the stars have statistically similar spot distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a simple probabilistic model using `pymc3` and solve for the five quantities above: the spot radius, the mode and standard deviation of the spot latitude, the spot contrast, and the number of spots. We'll place uniform priors on everything except for the latitude mode `mu`, on which we'll place an isotropic prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starry_process import StarryProcess\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with pm.Model() as model:\n",
    "\n",
    "    # For use later\n",
    "    varnames = [\"r\", \"mu\", \"sigma\", \"c\", \"n\"]\n",
    "\n",
    "    # Spot latitude params. Isotropic prior on the mode\n",
    "    # and uniform prior on the standard deviation\n",
    "    u = pm.Uniform(\"u\", 0, 1)\n",
    "    mu = 90 - tt.arccos(u) * 180 / np.pi\n",
    "    pm.Deterministic(\"mu\", mu)\n",
    "    sigma = pm.Uniform(\"sigma\", 1.0, 20.0)\n",
    "\n",
    "    # Spot radius (uniform prior)\n",
    "    r = pm.Uniform(\"r\", 10.0, 30.0)\n",
    "\n",
    "    # Spot contrast & number of spots (uniform prior)\n",
    "    c = pm.Uniform(\"c\", 0.0, 0.5, testval=0.1)\n",
    "    n = pm.Uniform(\"n\", 1.0, 30.0, testval=5)\n",
    "\n",
    "    # Instantiate the GP\n",
    "    sp = StarryProcess(r=r, mu=mu, sigma=sigma, c=c, n=n)\n",
    "\n",
    "    # Compute the log likelihood\n",
    "    lnlike = sp.log_likelihood(t, flux, ferr ** 2, p=1.0)\n",
    "    pm.Potential(\"lnlike\", lnlike)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "\n",
    "   Note that we explicitly provide a small ``testval`` for both the spot contrast and the number of spots. Otherwise, the initial value assumed in the optimization for both those quantities is the midpoint of the bounds (`c = 0.5` and `n = 25.5`). That corresponds to a *lot* of *very dark* spots, which results in very high variance in the flux -- too high, in fact, for the normalized Gaussian process to model! We discusss this in more detail in the paper. To avoid initializing the sampler in a bad region of parameter space, we provide a starting point that is guaranteed to lead to a finite log likelihood value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've declared our model, we *could* go on to sample it using `pymc3`, e.g.,\n",
    "\n",
    "```python\n",
    "with model:\n",
    "    trace = pm.sample(\n",
    "        tune=1000,\n",
    "        draws=2000,\n",
    "        cores=2,\n",
    "        chains=2,\n",
    "        init=\"adapt_full\",\n",
    "        target_accept=0.9,\n",
    "    )\n",
    "```\n",
    "\n",
    "But in practice, it can be *very difficult* for the default Hamiltonian Monte Carlo algorithm to sample the posterior in these kinds of problems. There is probably some intelligent way to reparametrize the problem to make HMC sample the posterior efficiently, but we haven't yet found it! Running the `pymc3` sampler above generally takes a *really* long time$-$it can take minutes to draw a single sample!\n",
    "\n",
    "We suspect that this is because the curvature of the posterior can be extremely variable, so methods that rely on first derivatives of the log probability (like HMC) struggle a lot. We've obtained far better performance using samplers that don't rely on gradient evaluations (which are rather slow to compute anyways). In our paper, we used Nested Sampling (implemented in the `dynesty` package), but here we'll use plain vanilla Markov Chain Monte Carlo (MCMC) as\n",
    "implemented in the `emcee` package.\n",
    "\n",
    "To use `emcee`, we need a function that returns the log probability at a point. We could compile this ourselves from the `log_likelihood` method of our `StarryProcess` instance (plus a log prior term) using `theano`, but `starry_process` actually implements a simple MCMC interface to do this for us.\n",
    "\n",
    "Let's instantiate this interface, which automatically compiles the `pymc3` model we defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starry_process import MCMCInterface\n",
    "\n",
    "with model:\n",
    "    mci = MCMCInterface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we can do with this interface object is to optimize the log probability function. This will get us the MAP (maximum a posteriori) solution, which is usually a good starting point for MCMC. Since we have so many light curves in our ensemble, it is actually also a good point estimate of the spot parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mci.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `optimize` method wraps a method with the same name in the `pymc3_ext` package (which in turn wraps `scipy.optimize.minimize`), so check those out for accepted keywords.\n",
    "\n",
    "The optimization should have run in about a minute, which is fairly fast thanks to the availability of the gradient. (Even though HMC struggles, the gradient is still useful for optimization!)\n",
    "\n",
    "Let's check out the solution vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If these numbers don't look right to you, it's because they do *not* correspond to our spot parameters `r`, `mu`, `sigma`, etc. Instead, they are quantities in the parametrization used internally in `pymc3`, which maps our priors onto a space with infinite support. This makes it easy to sample over them, since we don't need to specify hard bounds for any of them (they can take on any finite real value).\n",
    "\n",
    "We're going to run our sampler in this transformed space (in which our log probability function is defined) and transform back at the end. But we can take a peek at what they actually correspond to right now to check that our optimization worked correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames = [\"r\", \"mu\", \"sigma\", \"c\", \"n\"]\n",
    "mci.transform(x, varnames=varnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare these values to the true values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        \"\"\"\n",
    "| parameter | description | true value | MAP value\n",
    "| - | :- | :-: | :-:\n",
    "| `r` | mean radius in degrees | `{r}` | `{{:.2f}}` \n",
    "| `mu` | latitude distribution mode in degrees | `{mu}`| `{{:.2f}}` \n",
    "| `sigma` | latitude distribution standard deviation in degrees | `{sigma}`| `{{:.2f}}` \n",
    "| `c` | fractional spot contrast | `{c}`| `{{:.4f}}` \n",
    "| `n` | number of spots | `{n}`| `{{:.2f}}` \n",
    "\"\"\".format(\n",
    "            **truths\n",
    "        ).format(\n",
    "            *mci.transform(x, varnames=varnames)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! It seems that we correctly inferred all the properties (except for, maybe, the total number of spots; this quantity is generally not an observable in single-band photometry!)\n",
    "\n",
    "But we don't have any sense of the uncertainties on these quantities. For that, we need to run MCMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is define the initial point for each of the MCMC walkers (we'll use `30`). We can call the `get_initial_state` method to draw samples close to the `MAP` solution we obtained in the previous step. Note that this makes use of the estimate of the inverse Hessian matrix (returned by the optimizer), which can be somewhat numerically unstable. If this doesn't work, try specifying a small variance `var` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwalkers = 30\n",
    "p0 = mci.get_initial_state(nwalkers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to run MCMC. We'll instantiate a sampler and pass in `mci.logp` as our log probability$-$this is the automatically compiled function mentioned above. Since this notebook is running on GitHub Actions, we'll run the chains for a measly `500` steps (although you should run it for much longer if you can!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "# Number of parameters\n",
    "ndim = p0.shape[1]\n",
    "\n",
    "# Instantiate the sampler\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, mci.logp)\n",
    "\n",
    "# Run the chains\n",
    "np.random.seed(0)\n",
    "nsteps = 500\n",
    "_ = sampler.run_mcmc(p0, nsteps, progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our chains, recalling that everything is still in the *internal* parametrization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the walkers\n",
    "fig, ax = plt.subplots(ndim, figsize=(8, 8), sharex=True)\n",
    "for j in range(ndim):\n",
    "    for k in range(nwalkers):\n",
    "        ax[j].plot(sampler.chain[k, :, j], \"C0-\", lw=1, alpha=0.3)\n",
    "    ax[j].set_ylabel(\"param {}\".format(j + 1))\n",
    "ax[-1].set_xlabel(\"iteration\")\n",
    "fig.align_ylabels(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. note::\n",
    "\n",
    "    Remember that we only ran these chains for 500 steps since we're doing this on GitHub Actions. They're definitely not converged! When you're analyzing real data, make sure to run the chains for much longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of the first `100` samples as burn-in and flatten everything into an array of shape `(nsamples, ndim)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin = 100\n",
    "samples = sampler.chain[:, burnin:, :].reshape(-1, ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's transform to our desired parametrization using the `transform` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = mci.transform(samples, varnames=varnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can view the posteriors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corner import corner\n",
    "\n",
    "corner(\n",
    "    samples,\n",
    "    labels=varnames,\n",
    "    truths=[truths[name] for name in varnames],\n",
    "    range=((10, 30), (0, 90), (1, 20), (0, 0.5), (1, 30)),\n",
    "    truth_color=\"C1\",\n",
    "    bins=100,\n",
    "    smooth=2,\n",
    "    smooth1d=2,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Again, we seem to have correctly inferred everything except the number of spots, which is very unconstrained."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
